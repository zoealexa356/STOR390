---
title: "HW 2 Zoe Werner"
author: "Zoe Werner"
date: "09/24/2024"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]


```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)

#STUDENT INPUT


trained_knn <- knn(iris_train, iris_test, iris_target_category, k=5)
trained_knn
contingency_table <- table(trained_knn, iris_test_category)
contingency_table

summary(iris_test_category)
summary(iris_target_category)

```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  


*In class, the training and testing sets were chosen by a percentage, rather then by specific points in the data set. In the above code, the 'subset' function is a collection of specific, chosen points that are an unequal representation of the data. 'Subset' mainly consisted of data from setosa and virginica since 45 points were selected in the beginning of the set and 41 points at the end, with only 14 selections in the middle of the data set. This degraded the classification observations and increased the classification error rate. Therefore, the training data is not representative of the entire data set and increased the classification error rate. To decrease the classification error rate, we could select a percentage of the data as the subset so the sample is random rather then hand-selected. * 

#

Choice of $K$ can also influence this classifier.  Why would choosing $K = 6$ not be advisable for this data? 

*Choosing $K=6$ is not advisable because there are 3 categories (setosa, versicolor, and virginica) and 6 is divisible by 3. Choosing a K value that is indivisible by classes and is an odd number ensures that there will be a majority class. If K is divisible, it degrades KNN's performance.*

# 


Build a github repository to store your homework assignments.  Share the link in this file.  

*https://github.com/zoealexa356/STOR390/tree/b8e2b80cd99fc8ad9eae60367cdecf1785f73dac*

