---
title: "HW 3"
author: "Andy Ackerman -- completed by Zoe Werner"
date: "11/27/2023 -- completed by 11/8/2024"
output: 
  html_document:
    number_sections: true
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 

$E[(X-E[X])^2]$ 
$E[X^2-((2)(X)(E[X]))+E[X]^2]$ First, I distributed the squared property onto each variable in the parenthesis.
$E[X^2]-E[(2)(X)(E[X])]+E[E[X]^2]$ Next, I distributed the expectation to each section.
$E[X^2]-(2(E[X])(E[X]))+(E[X])^2$ Because of the linearity of expectations, I pulled the constant (2(E[X])) out of the brackets which left me with -(2(E[X])(E[X])). 
$E[X^2]-2((E[X])^2)+(E[X])^2$ I simplified further, condensing the middle section to -2((E[X])^2). 
$E[X^2]-(E[X])^2$ I combined -2((E[X])^2) and (E[X])^2, which left me with -(E[X])^2. 


# 


In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)
train=sample(200,100)
svmfit = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, dat[train,])


```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 10000)
plot(svmfit, dat[train,])


```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*OVERFITTING*
Increasing the cost(C) can lead to overfitting of the training data, which will then become too specific to be accurate for the testing data. It is important to maintain a balance when deciding the C value for an SVM model. If a smaller C value is resulting in a large number of misclassifications, increasing the C value may help alleviate this. However, you must find a balance. If the C value is too high, it will minimize misclassifications, but the model may be fit too closely to the training data. In this example, we increased the C value by 10000%, which is a drastic increase and is likely too closely fit to the testing data, meaning it will be too specific to be accurate in regards to the testing data. There must be a level of margin generality while still maximizing the accuracy of classifications.  

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
table(true=dat[-train,"y"], pred=predict(svmfit, newdata=dat[-train,]))
```

Yes, there is disparity in the classification results. The svm predicted 14 total wrong labels out of the 100 observations, so it is 86% accurate. 

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
sum(dat[train,3] == 2)/100

```

*29 percent is quite close to the underlying proportion.  This disparity seems to be less an artifact of imbalance between training and testing and more just overfitting the decision boundary.  If we have such an irregular decision boundary, even representative training data may not yield an unbiased model.*

Since the proportion of class 2 is 29%, which is moderately close to the 25% underlying proportion of actual class 2, it is less likely that the displarity is due to an imbalnace in the training/testing partition. Most likely, the cost(C) variable was too high and led to overfitting of the data. Since overfitting leads to a tight margin and more emphasis on accuracy, it is likely that the model was unable to accurately predict the testing data becuase the model was too closely fit to the training data. 
  

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1,2,3,4)))
summary(tune.out)


```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*We now appear to not be overfitting as extremely as our decision boundary is likely not as irregular and our error rate is at a minimum (even on the testing set).  However, because the original data had an imbalance in the classes, our classifier also misclassifies class 1 as class 2 much more often than the other way around.  This is not necessarily a short-coming as much as an artifact of the underlying data, but it is worth being aware of. *



# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}

for (i in 1:length(heart$class)) {
  if (heart$class[i] > 0){
    heart$class[i] = 1
  }
}

heart$class = as.factor(heart$class)

```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
train=sample(1:nrow(heart), 240)

tree.heart = tree(class~., heart, subset=train)
plot(tree.heart)
text(tree.heart, pretty=0)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}

tree.pred = predict(tree.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))
1-(28+18)/57

```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(101)
cv.heart = cv.tree(tree.heart, FUN = prune.misclass)
cv.heart

plot(cv.heart$size, cv.heart$dev, type = "b")

#three or four will probably do here
prune.heart = prune.misclass(tree.heart, best =3)
#figure out why this is above 100 percent and tell students in lecture
plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))
1-((26+17)/57)

```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*We have sacrificed a marginal amount of classification rate (about 4%) for a significantly more interpretable tree.  It seems that `thal` is the most influential variable followed by `ca` and `cp`. *

## 

Discuss the ways a decision tree could manifest algorithmic bias.  